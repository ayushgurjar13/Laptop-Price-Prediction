{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5b067e",
   "metadata": {},
   "source": [
    "# ðŸ•·ï¸ Web Scraping Code for Laptop Data\n",
    "\n",
    "This notebook contains multiple web scraping implementations to collect laptop data from Flipkart.\n",
    "\n",
    "## ðŸ“‹ Contents:\n",
    "1. **Cell 1**: Basic scraper with pagination (up to 10 pages)\n",
    "2. **Cell 2**: Enhanced scraper with improved selectors (up to 15 pages)\n",
    "3. **Cell 3**: Limited scraper with row limit (500 products max)\n",
    "4. **Cell 4**: Data inspection and statistics\n",
    "\n",
    "## âš™ï¸ Requirements:\n",
    "- selenium\n",
    "- beautifulsoup4\n",
    "- webdriver-manager\n",
    "- pandas\n",
    "\n",
    "## ðŸ“ Notes:\n",
    "- The code uses Selenium 4 syntax (updated from deprecated methods)\n",
    "- Headless mode is enabled for background execution\n",
    "- Multiple fallback selectors for better compatibility\n",
    "- Error handling for robust scraping\n",
    "- Automated pagination with safety limits\n",
    "\n",
    "## ðŸš€ Usage:\n",
    "Run any of the first three cells to scrape data. Each implementation has different features:\n",
    "- Choose Cell 1 for basic scraping\n",
    "- Choose Cell 2 for more reliable scraping\n",
    "- Choose Cell 3 if you want to limit the number of products\n",
    "\n",
    "After scraping, run Cell 4 to inspect the collected data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db42737",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--window-size=1920x1080\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Set window size to handle elements properly\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Setup the driver\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mService(ChromeDriverManager()\u001b[38;5;241m.\u001b[39minstall()), options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# URL of the Flipkart page you want to scrape\u001b[39;00m\n\u001b[0;32m     20\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com/search?q=laptop\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\chrome.py:40\u001b[0m, in \u001b[0;36mChromeDriverManager.install\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     driver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_driver_binary_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver)\n\u001b[0;32m     41\u001b[0m     os\u001b[38;5;241m.\u001b[39mchmod(driver_path, \u001b[38;5;241m0o755\u001b[39m)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\core\\manager.py:40\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n\u001b[0;32m     39\u001b[0m os_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_os_type()\n\u001b[1;32m---> 40\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_manager\u001b[38;5;241m.\u001b[39mdownload_file(driver\u001b[38;5;241m.\u001b[39mget_driver_download_url(os_type))\n\u001b[0;32m     41\u001b[0m binary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_manager\u001b[38;5;241m.\u001b[39msave_file_to_cache(driver, file)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\drivers\\chrome.py:32\u001b[0m, in \u001b[0;36mChromeDriver.get_driver_download_url\u001b[1;34m(self, os_type)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_driver_download_url\u001b[39m(\u001b[38;5;28mself\u001b[39m, os_type):\n\u001b[1;32m---> 32\u001b[0m     driver_version_to_download \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_driver_version_to_download()\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# For Mac ARM CPUs after version 106.0.5249.61 the format of OS type changed\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# to more unified \"mac_arm64\". For newer versions, it'll be \"mac_arm64\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# by default, for lower versions we replace \"mac_arm64\" to old format - \"mac64_m1\".\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(driver_version_to_download) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m106.0.5249.61\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\core\\driver.py:48\u001b[0m, in \u001b[0;36mDriver.get_driver_version_to_download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_latest_release_version()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\drivers\\chrome.py:59\u001b[0m, in \u001b[0;36mChromeDriver.get_latest_release_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m determined_browser_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(determined_browser_version) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m115\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     58\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://googlechromelabs.github.io/chrome-for-testing/latest-patch-versions-per-build.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 59\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_client\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     60\u001b[0m     response_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     61\u001b[0m     determined_browser_version \u001b[38;5;241m=\u001b[39m response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget(determined_browser_version)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\core\\http.py:32\u001b[0m, in \u001b[0;36mWDMHttpClient.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     33\u001b[0m             url\u001b[38;5;241m=\u001b[39murl, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_verify, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mConnectionError:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mConnectionError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not reach host. Are you offline?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    792\u001b[0m     conn,\n\u001b[0;32m    793\u001b[0m     method,\n\u001b[0;32m    794\u001b[0m     url,\n\u001b[0;32m    795\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    796\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    797\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    798\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    799\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    800\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    801\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    802\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    804\u001b[0m )\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    807\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:1386\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1385\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1386\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1388\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup Chrome options\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in headless mode (without GUI)\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--window-size=1920x1080')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Setup the driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# URL of the Flipkart page you want to scrape\n",
    "url = 'https://www.flipkart.com/search?q=laptop'\n",
    "\n",
    "# Navigate to the page\n",
    "driver.get(url)\n",
    "time.sleep(5)  # Wait for page to load\n",
    "\n",
    "# Initialize an empty list to store product details\n",
    "products = []\n",
    "\n",
    "# Function to extract product details from the current page\n",
    "def extract_product_details(soup):\n",
    "    cards = soup.find_all('div', class_='tUxRFH')\n",
    "    \n",
    "    if not cards:\n",
    "        print(f\"No cards found. Trying alternative selectors...\")\n",
    "        cards = soup.find_all('div', {'data-id': True})\n",
    "    \n",
    "    print(f\"Found {len(cards)} products on this page\")\n",
    "\n",
    "    for card in cards:\n",
    "        try:\n",
    "            model = card.find('div', {'class': \"KzDlHZ\"})\n",
    "            model = model.text.strip() if model else 'N/A'\n",
    "            \n",
    "            star = card.find('div', {'class': \"XQDdHH\"})\n",
    "            star = star.text.strip() if star else 'No rating'\n",
    "            \n",
    "            rating_elem = card.find('span', {'class': \"Wphh3N\"})\n",
    "            if rating_elem:\n",
    "                rating_text = rating_elem.text.strip()\n",
    "                parts = rating_text.split('&')\n",
    "                num_ratings = parts[0].strip() if len(parts) > 0 else 'No ratings'\n",
    "                reviews = parts[1].strip() if len(parts) > 1 else 'No reviews'\n",
    "            else:\n",
    "                num_ratings = 'No ratings'\n",
    "                reviews = 'No reviews'\n",
    "            \n",
    "            specs = card.find_all('li', {'class': 'rgWa7D'})\n",
    "            processor = specs[0].text.strip() if len(specs) > 0 else 'N/A'\n",
    "            ram = specs[1].text.strip() if len(specs) > 1 else 'N/A'\n",
    "            os = specs[2].text.strip() if len(specs) > 2 else 'N/A'\n",
    "            storage = specs[3].text.strip() if len(specs) > 3 else 'N/A'\n",
    "            size = specs[4].text.strip() if len(specs) > 4 else 'N/A'\n",
    "            \n",
    "            price_elem = card.find('div', {'class': 'Nx9bqj'})\n",
    "            price = price_elem.text.strip() if price_elem else 'Price not available'\n",
    "\n",
    "            products.append({\n",
    "                'Model': model,\n",
    "                'Star Rating': star,\n",
    "                'Number of Ratings': num_ratings,\n",
    "                'Number of Reviews': reviews,\n",
    "                'Processor': processor,\n",
    "                'RAM': ram,\n",
    "                'OS': os,\n",
    "                'Storage': storage,\n",
    "                'Size': size,\n",
    "                'Price': price\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing card: {e}\")\n",
    "\n",
    "# Scrape multiple pages (limit to prevent overloading)\n",
    "max_pages = 10\n",
    "page_count = 0\n",
    "\n",
    "while page_count < max_pages:\n",
    "    print(f\"Scraping page {page_count + 1}...\")\n",
    "    \n",
    "    # Retrieve page source\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Extract product details from the current page\n",
    "    extract_product_details(soup)\n",
    "\n",
    "    try:\n",
    "        # Find next button using Selenium 4 syntax\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//a[@class='_9QVEpD' and contains(., 'Next')]\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", next_button)\n",
    "        time.sleep(1)\n",
    "        next_button.click()\n",
    "        time.sleep(5)  # Wait for the next page to load\n",
    "        page_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"No more pages or unable to find the Next button: {e}\")\n",
    "        break\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert the list of products into a DataFrame\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('flipkart_laptops.csv', index=False)\n",
    "\n",
    "print(f\"Data has been saved to 'flipkart_laptops.csv'. Total products scraped: {len(products)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c8e2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages or unable to find the Next button.\n",
      "Data has been saved to 'flipkart_gaming_laptops.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup Chrome options with improved settings\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--window-size=1920x1080')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "\n",
    "# Setup the driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# URL of the Flipkart page you want to scrape\n",
    "url = 'https://www.flipkart.com/search?q=laptop'\n",
    "\n",
    "# Navigate to the page\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Initialize an empty list to store product details\n",
    "products = []\n",
    "\n",
    "# Function to extract product details from the current page\n",
    "def extract_product_details(soup):\n",
    "    # Try multiple selectors for better compatibility\n",
    "    cards = soup.find_all('div', class_='tUxRFH')\n",
    "    \n",
    "    if not cards:\n",
    "        cards = soup.find_all('div', {'data-id': True})\n",
    "    \n",
    "    print(f\"Found {len(cards)} products on this page\")\n",
    "\n",
    "    for card in cards:\n",
    "        try:\n",
    "            # Extract model name\n",
    "            model_elem = card.find('div', {'class': \"KzDlHZ\"}) or card.find('a', {'class': 's1Q9rs'})\n",
    "            model = model_elem.text.strip() if model_elem else 'N/A'\n",
    "            \n",
    "            # Extract star rating\n",
    "            star_elem = card.find('div', {'class': \"XQDdHH\"})\n",
    "            star = star_elem.text.strip() if star_elem else 'No rating'\n",
    "            \n",
    "            # Extract ratings and reviews\n",
    "            rating_elem = card.find('span', {'class': \"Wphh3N\"}) or card.find('span', {'class': 'hG7V+4'})\n",
    "            if rating_elem:\n",
    "                rating_text = rating_elem.text.replace('\\xa0', ' ')\n",
    "                parts = rating_text.split('&')\n",
    "                num_ratings = parts[0].strip() if len(parts) > 0 else 'No ratings'\n",
    "                reviews = parts[1].strip() if len(parts) > 1 else 'No reviews'\n",
    "            else:\n",
    "                num_ratings = 'No ratings'\n",
    "                reviews = 'No reviews'\n",
    "            \n",
    "            # Extract specifications\n",
    "            specs = card.find_all('li', {'class': 'rgWa7D'}) or card.find_all('li')\n",
    "            processor = specs[0].text.strip() if len(specs) > 0 else 'N/A'\n",
    "            ram = specs[1].text.strip() if len(specs) > 1 else 'N/A'\n",
    "            os = specs[2].text.strip() if len(specs) > 2 else 'N/A'\n",
    "            storage = specs[3].text.strip() if len(specs) > 3 else 'N/A'\n",
    "            size = specs[4].text.strip() if len(specs) > 4 else 'N/A'\n",
    "            \n",
    "            # Extract price\n",
    "            price_elem = card.find('div', {'class': 'Nx9bqj'}) or card.find('div', {'class': '_30jeq3'})\n",
    "            price = price_elem.text.strip() if price_elem else 'Price not available'\n",
    "\n",
    "            products.append({\n",
    "                'Model': model,\n",
    "                'Star Rating': star,\n",
    "                'Number of Ratings': num_ratings,\n",
    "                'Number of Reviews': reviews,\n",
    "                'Processor': processor,\n",
    "                'RAM': ram,\n",
    "                'OS': os,\n",
    "                'Storage': storage,\n",
    "                'Size': size,\n",
    "                'Price': price\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing card: {e}\")\n",
    "            continue\n",
    "\n",
    "# Scrape multiple pages\n",
    "max_pages = 15\n",
    "page_count = 0\n",
    "\n",
    "while page_count < max_pages:\n",
    "    print(f\"Scraping page {page_count + 1}...\")\n",
    "    \n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    extract_product_details(soup)\n",
    "\n",
    "    try:\n",
    "        # Updated method to find and click next button\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//a[contains(@class, '_9QVEpD') and span[text()='Next']]\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(5)\n",
    "        page_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Pagination ended or error: {e}\")\n",
    "        break\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df = pd.DataFrame(products)\n",
    "df.to_csv('flipkart_laptops_extended.csv', index=False)\n",
    "\n",
    "print(f\"Scraping complete! Total products: {len(products)}\")\n",
    "print(f\"Data saved to 'flipkart_laptops_extended.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative scraper with row limit and error handling\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Setup Chrome options\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--window-size=1920x1080')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "# Setup the driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://www.flipkart.com/search?q=laptop'\n",
    "\n",
    "# Navigate to the page\n",
    "driver.get(url)\n",
    "time.sleep(5)\n",
    "\n",
    "# Initialize list and set row limit\n",
    "products = []\n",
    "max_rows = 500  # Limit for the number of products to scrape\n",
    "\n",
    "# Function to extract product details from the current page\n",
    "def extract_product_details(soup):\n",
    "    cards = soup.find_all('div', class_='tUxRFH')\n",
    "    \n",
    "    if not cards:\n",
    "        cards = soup.find_all('div', {'data-id': True})\n",
    "    \n",
    "    print(f\"Found {len(cards)} products on this page\")\n",
    "\n",
    "    for card in cards:\n",
    "        # Check if we've reached the limit\n",
    "        if len(products) >= max_rows:\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            model_elem = card.find('div', {'class': \"KzDlHZ\"})\n",
    "            model = model_elem.text.strip() if model_elem else 'N/A'\n",
    "            \n",
    "            star_elem = card.find('div', {'class': \"XQDdHH\"})\n",
    "            star = star_elem.text.strip() if star_elem else 'No rating'\n",
    "            \n",
    "            rating_elem = card.find('span', {'class': \"Wphh3N\"})\n",
    "            if rating_elem:\n",
    "                rating_text = rating_elem.text.replace('\\xa0', ' ')\n",
    "                parts = rating_text.split('&')\n",
    "                num_ratings = parts[0].strip() if len(parts) > 0 else 'No ratings'\n",
    "                reviews = parts[1].strip() if len(parts) > 1 else 'No reviews'\n",
    "            else:\n",
    "                num_ratings = 'No ratings'\n",
    "                reviews = 'No reviews'\n",
    "            \n",
    "            specs = card.find_all('li', {'class': 'rgWa7D'})\n",
    "            processor = specs[0].text.strip() if len(specs) > 0 else 'N/A'\n",
    "            ram = specs[1].text.strip() if len(specs) > 1 else 'N/A'\n",
    "            os = specs[2].text.strip() if len(specs) > 2 else 'N/A'\n",
    "            storage = specs[3].text.strip() if len(specs) > 3 else 'N/A'\n",
    "            size = specs[4].text.strip() if len(specs) > 4 else 'N/A'\n",
    "            \n",
    "            price_elem = card.find('div', {'class': 'Nx9bqj'})\n",
    "            price = price_elem.text.strip() if price_elem else 'Price not available'\n",
    "\n",
    "            products.append({\n",
    "                'Model': model,\n",
    "                'Star Rating': star,\n",
    "                'Number of Ratings': num_ratings,\n",
    "                'Number of Reviews': reviews,\n",
    "                'Processor': processor,\n",
    "                'RAM': ram,\n",
    "                'OS': os,\n",
    "                'Storage': storage,\n",
    "                'Size': size,\n",
    "                'Price': price\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing card: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Scrape multiple pages\n",
    "page_count = 0\n",
    "\n",
    "while len(products) < max_rows:\n",
    "    print(f\"Scraping page {page_count + 1}... (Products collected: {len(products)}/{max_rows})\")\n",
    "    \n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Extract products and check if we should continue\n",
    "    should_continue = extract_product_details(soup)\n",
    "    \n",
    "    if not should_continue or len(products) >= max_rows:\n",
    "        print(f\"Reached the maximum row limit of {max_rows}.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Find and click next button\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//a[contains(@class, '_9QVEpD')]//span[contains(text(), 'Next')]\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "        time.sleep(5)\n",
    "        page_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Pagination ended or error: {e}\")\n",
    "        break\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "# Remove duplicates if any\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('flipkart_laptops_limited.csv', index=False)\n",
    "\n",
    "print(f\"\\nScraping complete!\")\n",
    "print(f\"Total products collected: {len(df)}\")\n",
    "print(f\"Data saved to 'flipkart_laptops_limited.csv'\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7e4e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--window-size=1920x1080\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Set window size to handle elements properly\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Setup the driver\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mService(ChromeDriverManager()\u001b[38;5;241m.\u001b[39minstall()), options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# URL of the Flipkart page you want to scrape\u001b[39;00m\n\u001b[0;32m     19\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com/search?q=laptops&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\chrome.py:40\u001b[0m, in \u001b[0;36mChromeDriverManager.install\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     driver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_driver_binary_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver)\n\u001b[0;32m     41\u001b[0m     os\u001b[38;5;241m.\u001b[39mchmod(driver_path, \u001b[38;5;241m0o755\u001b[39m)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\core\\manager.py:40\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n\u001b[0;32m     39\u001b[0m os_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_os_type()\n\u001b[1;32m---> 40\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_manager\u001b[38;5;241m.\u001b[39mdownload_file(driver\u001b[38;5;241m.\u001b[39mget_driver_download_url(os_type))\n\u001b[0;32m     41\u001b[0m binary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_manager\u001b[38;5;241m.\u001b[39msave_file_to_cache(driver, file)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\core\\download_manager.py:32\u001b[0m, in \u001b[0;36mWDMDownloadManager.download_file\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     30\u001b[0m log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDriver downloading response is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_filename_from_url(url)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m File(response, file_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\webdriver_manager\\core\\file_manager.py:12\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, stream, file_name)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream, file_name):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stream \u001b[38;5;241m=\u001b[39m stream\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_name \u001b[38;5;241m=\u001b[39m file_name\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:936\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 936\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    939\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[0;32m    881\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    811\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 814\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data inspection and basic statistics\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the scraped data\n",
    "try:\n",
    "    df = pd.read_csv('flipkart_laptops.csv')\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"DATA OVERVIEW\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nTotal records: {len(df)}\")\n",
    "    print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "    print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FIRST 10 ROWS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.head(10))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BASIC STATISTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Save a cleaned version\n",
    "    df_clean = df.dropna()\n",
    "    df_clean.to_csv('uncleaned_file.csv', index=False)\n",
    "    print(f\"\\nâœ“ Data saved to 'uncleaned_file.csv'\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV file not found. Please run the scraping cells first!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46643d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
